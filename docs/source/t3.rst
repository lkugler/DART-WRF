Tutorial 3: Cycle forecast and assimilation
###########################################

**Goal**: To run a cycled data assimilation experiment.
This page explains the ``DART-WRF/cycled.py`` script.


.. code-block:: python

    time0 = dt.datetime(2008, 7, 30, 11)
    time_end = dt.datetime(2008, 7, 30, 12, 30)

    id = None
    cfg = Config(name='exp_nat250_VIS_mCF-3_oe2_inf3_test_b',
        model_dx = 2000,
        ensemble_size = ensemble_size,
        dart_nml = dart_nml,
        geo_em_forecast = '/jetfs/home/lkugler/data/sim_archive/geo_em.d01.nc.2km_200x200',
        time = time0,
        
        assimilate_these_observations = assimilate_these_observations,
        assimilate_cloudfractions = assimilate_cloudfractions,
        #CF_config=CF_config,
        
        assimilate_existing_obsseq = False,
        #nature_wrfout_pattern = '/jetfs/home/lkugler/data/sim_archive/nat_250m_1600x1600x100/*/1/wrfout_d01_%Y-%m-%d_%H_%M_%S',
        #geo_em_nature = '/jetfs/home/lkugler/data/sim_archive/geo_em.d01.nc.2km_200x200',
        #geo_em_nature = '/jetfs/home/lkugler/data/sim_archive/geo_em.d01.nc.250m_1600x1600',
        
        update_vars = ['U', 'V', 'W', 'THM', 'PH', 'MU', 'QVAPOR', 'QCLOUD', 'QICE', 'QSNOW', 'PSFC'],
        #input_profile = '/jetfs/home/lkugler/data/sim_archive/nat_250m_1600x1600x100/2008-07-30_08:00/1/input_sounding',
        verify_against = 'nat_250m_blockavg2km',
        **cluster_defaults)

    w = WorkFlows(cfg)
    w.prepare_WRFrundir(cfg)

    # assimilate at these times
    timedelta_btw_assim = dt.timedelta(minutes=15)
    assim_times = pd.date_range(time0, time_end, freq=timedelta_btw_assim)

    # loop over assimilations
    for i, t in enumerate(assim_times):
        
        if i == 0:
            cfg.update(
                time = t,
                prior_init_time = dt.datetime(2008, 7, 30, 8),
                prior_valid_time = t,
                prior_path_exp = '/jetfs/home/lkugler/data/sim_archive/exp_nat250m_noDA_b/',)
        else:
            cfg.update(
                time = t,
                prior_init_time = t - dt.timedelta(minutes=15),
                prior_valid_time = t,
                prior_path_exp = cfg.dir_archive,)

        id = w.assimilate(cfg, depends_on=id)

        # 1) Set posterior = prior
        id = w.prepare_IC_from_prior(cfg, depends_on=id)

        # 2) Update posterior += updates from assimilation
        id = w.update_IC_from_DA(cfg, depends_on=id)
        
        cfg.update( WRF_start=t, 
                    WRF_end=t+timedelta_integrate, 
                    restart=True, 
                    restart_interval=restart_interval,
                    hist_interval_s=300,
        )

        # 3) Run WRF ensemble
        id = w.run_WRF(cfg, depends_on=id)


Using SLURM
-----------

SLURM is a job scheduler which is required when running 
resource-intensive tasks on a cluster.

It is very easy to use the SLURM job scheduler.
Set parameter ``use_slurm = True`` of :class:`dartwrf.utils.Config` 
and customize ``max_nproc`` (number of processors used at once in total), 
``max_nproc_for_each_ensemble_member`` (number of processors used for each ensemble member),
Configure the amount of resources each job gets allocated by SLURM
by modifying the script ``DART-WRF/dartwrf/workflows.py`` for each workflow method you use.

Lastly, call workflow methods with the keyword argument ``depends_on=id``.
This tells each job to wait on another job's completion. 
For example:

.. code-block:: python

    id = None
    id = w.assimilate(cfg, depends_on=id)
    id = w.prepare_IC_from_prior(cfg, depends_on=id)
    id = w.update_IC_from_DA(cfg, depends_on=id)
